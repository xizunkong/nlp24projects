{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b007d09-ee22-4e0e-8a83-23b5dbaf5f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at F:\\python\\SVM_for_FakeNewsDetection-master\\mBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 714/714 [03:02<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 257.4466, Accuracy: 0.8462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 714/714 [03:13<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 177.4807, Accuracy: 0.8997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 714/714 [03:16<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 134.7736, Accuracy: 0.9307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 714/714 [03:20<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 133.7548, Accuracy: 0.9346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 714/714 [03:22<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 455.1491, Accuracy: 0.6646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 714/714 [03:28<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 464.6364, Accuracy: 0.6536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 714/714 [03:18<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 464.8289, Accuracy: 0.6562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 714/714 [03:14<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 465.1363, Accuracy: 0.6561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 714/714 [03:12<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 465.1454, Accuracy: 0.6544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 714/714 [03:15<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 465.2360, Accuracy: 0.6551\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.65      1.00      0.79      1854\n",
      "        Real       0.00      0.00      0.00      1002\n",
      "\n",
      "    accuracy                           0.65      2856\n",
      "   macro avg       0.32      0.50      0.39      2856\n",
      "weighted avg       0.42      0.65      0.51      2856\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 读取数据\n",
    "train_data = pd.read_csv(\"mediaeval-2015-trainingset.txt\", sep=\"\\t\")\n",
    "test_data = pd.read_csv(\"mediaeval-2015-testset.txt\", sep=\"\\t\")\n",
    "\n",
    "# 提取文本和标签\n",
    "train_texts = train_data[\"tweetText\"].tolist()\n",
    "train_labels = train_data[\"label\"].apply(lambda x: 1 if x == \"real\" else 0).tolist()\n",
    "\n",
    "test_texts = test_data[\"tweetText\"].tolist()\n",
    "test_labels = test_data[\"label\"].apply(lambda x: 1 if x == \"real\" else 0).tolist()\n",
    "\n",
    "# 划分验证集\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 指定本地路径\n",
    "local_path = \"F:\\python\\SVM_for_FakeNewsDetection-master\\mBERT\"\n",
    "\n",
    "# 加载预训练的BERT分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(local_path)\n",
    "\n",
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# 创建数据集实例\n",
    "train_dataset = FakeNewsDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
    "val_dataset = FakeNewsDataset(val_texts, val_labels, tokenizer, max_length=128)\n",
    "test_dataset = FakeNewsDataset(test_texts, test_labels, tokenizer, max_length=128)\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 加载预训练的BERT模型（2分类）\n",
    "model = BertForSequenceClassification.from_pretrained(local_path, num_labels=2)\n",
    "\n",
    "# 将模型移动到GPU（如果可用）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 定义DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# 优化器\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 训练循环\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "            \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "            \"labels\": batch[\"labels\"].to(device),\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == inputs[\"labels\"]).sum().item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}, Accuracy: {correct / len(train_dataset):.4f}\")\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 评估函数\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "    return all_preds, all_labels\n",
    "\n",
    "# 在验证集上评估\n",
    "val_preds, val_labels = evaluate(model, val_loader)\n",
    "print(classification_report(val_labels, val_preds, target_names=[\"Fake\", \"Real\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e739d6da-52ba-49a1-bc26-930c102f1ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
